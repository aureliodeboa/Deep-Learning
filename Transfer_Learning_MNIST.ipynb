{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aureliodeboa/Deep-Learning/blob/main/Transfer_Learning_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0LSygkOSN9d"
      },
      "source": [
        "## Transfer Learning using MNIST data\n",
        "To illustrate the power and concept of transfer learning, we will train a CNN on just the digits 5,6,7,8,9.  Then we will train just the last layer(s) of the network on the digits 0,1,2,3,4 and see how well the features learned on 5-9 help with classifying 0-4.\n",
        "\n",
        "Adapted from https://github.com/fchollet/keras/blob/master/examples/mnist_transfer_cnn.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOnWenKxSN9j"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "drBML_O_SN9m"
      },
      "outputs": [],
      "source": [
        "#used to help some of the timing functions\n",
        "now = datetime.datetime.now"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "c2Tf88QgSN9n"
      },
      "outputs": [],
      "source": [
        "# set some parameters\n",
        "batch_size = 128\n",
        "num_classes = 5\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "GOZSJk_pSN9o"
      },
      "outputs": [],
      "source": [
        "# set some more parameters\n",
        "img_rows, img_cols = 28, 28\n",
        "filters = 32\n",
        "pool_size = 2\n",
        "kernel_size = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KTW3Yhn0SN9o"
      },
      "outputs": [],
      "source": [
        "## This just handles some variability in how the input data is loaded\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KH7XXNLhSN9p"
      },
      "outputs": [],
      "source": [
        "## To simplify things, write a function to include all the training steps\n",
        "## As input, function takes a model, training set, test set, and the number of classes\n",
        "## Inside the model object will be the state about which layers we are freezing and which we are training\n",
        "\n",
        "def train_model(model, train, test, num_classes):\n",
        "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
        "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
        "    x_train = x_train.astype('float32')\n",
        "    x_test = x_test.astype('float32')\n",
        "    x_train /= 255\n",
        "    x_test /= 255\n",
        "    print('x_train shape:', x_train.shape)\n",
        "    print(x_train.shape[0], 'train samples')\n",
        "    print(x_test.shape[0], 'test samples')\n",
        "\n",
        "    # convert class vectors to binary class matrices\n",
        "    y_train = keras.utils.to_categorical(train[1], num_classes)\n",
        "    y_test = keras.utils.to_categorical(test[1], num_classes)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='adadelta',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    t = now()\n",
        "    model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=(x_test, y_test))\n",
        "    print('Training time: %s' % (now() - t))\n",
        "\n",
        "    score = model.evaluate(x_test, y_test, verbose=0)\n",
        "    print('Test score:', score[0])\n",
        "    print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IFkAN8YjSN9q"
      },
      "outputs": [],
      "source": [
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# create two datasets: one with digits below 5 and one with 5 and above\n",
        "x_train_lt5 = x_train[y_train < 5]\n",
        "y_train_lt5 = y_train[y_train < 5]\n",
        "x_test_lt5 = x_test[y_test < 5]\n",
        "y_test_lt5 = y_test[y_test < 5]\n",
        "\n",
        "x_train_gte5 = x_train[y_train >= 5]\n",
        "y_train_gte5 = y_train[y_train >= 5] - 5\n",
        "x_test_gte5 = x_test[y_test >= 5]\n",
        "y_test_gte5 = y_test[y_test >= 5] - 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "F48majBySN9r"
      },
      "outputs": [],
      "source": [
        "# Define the \"feature\" layers.  These are the early layers that we expect will \"transfer\"\n",
        "# to a new problem.  We will freeze these layers during the fine-tuning process\n",
        "\n",
        "feature_layers = [\n",
        "    Conv2D(filters, kernel_size,\n",
        "           padding='valid',\n",
        "           input_shape=input_shape),\n",
        "    Activation('relu'),\n",
        "    Conv2D(filters, kernel_size),\n",
        "    Activation('relu'),\n",
        "    MaxPooling2D(pool_size=pool_size),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nUmVd5NMSN9s"
      },
      "outputs": [],
      "source": [
        "# Define the \"classification\" layers.  These are the later layers that predict the specific classes from the features\n",
        "# learned by the feature layers.  This is the part of the model that needs to be re-trained for a new problem\n",
        "\n",
        "classification_layers = [\n",
        "    Dense(128),\n",
        "    Activation('relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(num_classes),\n",
        "    Activation('softmax')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JPD3exb5SN9s"
      },
      "outputs": [],
      "source": [
        "# We create our model by combining the two sets of layers as follows\n",
        "model = Sequential(feature_layers + classification_layers)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVPBQYpTSN9t",
        "outputId": "4dd35879-75de-41fa-dd8a-74ea1d1f0042",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               589952    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600165 (2.29 MB)\n",
            "Trainable params: 600165 (2.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Let's take a look\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU_j5AGYSN9t",
        "outputId": "106a9c14-a64c-4a11-e3d6-9c6b281f5f9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (29404, 28, 28, 1)\n",
            "29404 train samples\n",
            "4861 test samples\n",
            "Epoch 1/20\n",
            "230/230 [==============================] - 3s 8ms/step - loss: 1.6161 - accuracy: 0.2151 - val_loss: 1.5917 - val_accuracy: 0.2331\n",
            "Epoch 2/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.5878 - accuracy: 0.2516 - val_loss: 1.5607 - val_accuracy: 0.3191\n",
            "Epoch 3/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.5587 - accuracy: 0.3132 - val_loss: 1.5290 - val_accuracy: 0.5182\n",
            "Epoch 4/20\n",
            "230/230 [==============================] - 2s 8ms/step - loss: 1.5288 - accuracy: 0.3849 - val_loss: 1.4947 - val_accuracy: 0.6863\n",
            "Epoch 5/20\n",
            "230/230 [==============================] - 2s 8ms/step - loss: 1.4976 - accuracy: 0.4473 - val_loss: 1.4563 - val_accuracy: 0.7455\n",
            "Epoch 6/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.4595 - accuracy: 0.5131 - val_loss: 1.4120 - val_accuracy: 0.7682\n",
            "Epoch 7/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.4156 - accuracy: 0.5673 - val_loss: 1.3605 - val_accuracy: 0.7778\n",
            "Epoch 8/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.3682 - accuracy: 0.6076 - val_loss: 1.3017 - val_accuracy: 0.7945\n",
            "Epoch 9/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.3115 - accuracy: 0.6414 - val_loss: 1.2358 - val_accuracy: 0.8058\n",
            "Epoch 10/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.2505 - accuracy: 0.6701 - val_loss: 1.1634 - val_accuracy: 0.8167\n",
            "Epoch 11/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.1863 - accuracy: 0.6877 - val_loss: 1.0875 - val_accuracy: 0.8280\n",
            "Epoch 12/20\n",
            "230/230 [==============================] - 2s 8ms/step - loss: 1.1183 - accuracy: 0.7098 - val_loss: 1.0104 - val_accuracy: 0.8377\n",
            "Epoch 13/20\n",
            "230/230 [==============================] - 2s 8ms/step - loss: 1.0460 - accuracy: 0.7265 - val_loss: 0.9341 - val_accuracy: 0.8457\n",
            "Epoch 14/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 0.9825 - accuracy: 0.7393 - val_loss: 0.8618 - val_accuracy: 0.8548\n",
            "Epoch 15/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 0.9172 - accuracy: 0.7530 - val_loss: 0.7945 - val_accuracy: 0.8597\n",
            "Epoch 16/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 0.8649 - accuracy: 0.7628 - val_loss: 0.7334 - val_accuracy: 0.8673\n",
            "Epoch 17/20\n",
            "230/230 [==============================] - 2s 9ms/step - loss: 0.8130 - accuracy: 0.7704 - val_loss: 0.6792 - val_accuracy: 0.8692\n",
            "Epoch 18/20\n",
            "230/230 [==============================] - 2s 10ms/step - loss: 0.7680 - accuracy: 0.7783 - val_loss: 0.6315 - val_accuracy: 0.8743\n",
            "Epoch 19/20\n",
            "230/230 [==============================] - 2s 8ms/step - loss: 0.7233 - accuracy: 0.7905 - val_loss: 0.5897 - val_accuracy: 0.8794\n",
            "Epoch 20/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 0.6893 - accuracy: 0.7944 - val_loss: 0.5533 - val_accuracy: 0.8819\n",
            "Training time: 0:00:41.864403\n",
            "Test score: 0.553270697593689\n",
            "Test accuracy: 0.8819172978401184\n"
          ]
        }
      ],
      "source": [
        "# Now, let's train our model on the digits 5,6,7,8,9\n",
        "\n",
        "train_model(model,\n",
        "            (x_train_gte5, y_train_gte5),\n",
        "            (x_test_gte5, y_test_gte5), num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oV97XUrSSN9u"
      },
      "source": [
        "### Freezing Layers\n",
        "Keras allows layers to be \"frozen\" during the training process.  That is, some layers would have their weights updated during the training process, while others would not.  This is a core part of transfer learning, the ability to train just the last one or several layers.\n",
        "\n",
        "Note also, that a lot of the training time is spent \"back-propagating\" the gradients back to the first layer.  Therefore, if we only need to compute the gradients back a small number of layers, the training time is much quicker per iteration.  This is in addition to the savings gained by being able to train on a smaller data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wSIRI11RSN9u"
      },
      "outputs": [],
      "source": [
        "# Freeze only the\n",
        "for l in feature_layers:\n",
        "    l.trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAlJeMIGSN9u"
      },
      "source": [
        "Observe below the differences between the number of *total params*, *trainable params*, and *non-trainable params*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAss62awSN9v",
        "outputId": "64002772-f598-4fcd-fbe7-62f50676cfa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               589952    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600165 (2.29 MB)\n",
            "Trainable params: 590597 (2.25 MB)\n",
            "Non-trainable params: 9568 (37.38 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUJyGtlXSN9v",
        "outputId": "f5e72693-b085-4a4c-c666-3284e604b110",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (30596, 28, 28, 1)\n",
            "30596 train samples\n",
            "5139 test samples\n",
            "Epoch 1/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 1.6569 - accuracy: 0.3582 - val_loss: 1.4881 - val_accuracy: 0.3913\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 1.4954 - accuracy: 0.4045 - val_loss: 1.3349 - val_accuracy: 0.4419\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 1.3578 - accuracy: 0.4539 - val_loss: 1.2028 - val_accuracy: 0.5198\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 1.2433 - accuracy: 0.5039 - val_loss: 1.0872 - val_accuracy: 0.6040\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 1.1422 - accuracy: 0.5600 - val_loss: 0.9890 - val_accuracy: 0.6739\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 1.0541 - accuracy: 0.6105 - val_loss: 0.9027 - val_accuracy: 0.7297\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.9778 - accuracy: 0.6591 - val_loss: 0.8271 - val_accuracy: 0.7682\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.9062 - accuracy: 0.6973 - val_loss: 0.7606 - val_accuracy: 0.8017\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.8494 - accuracy: 0.7318 - val_loss: 0.7025 - val_accuracy: 0.8272\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 1s 6ms/step - loss: 0.7928 - accuracy: 0.7605 - val_loss: 0.6505 - val_accuracy: 0.8459\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.7419 - accuracy: 0.7846 - val_loss: 0.6053 - val_accuracy: 0.8622\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 1s 6ms/step - loss: 0.7002 - accuracy: 0.8030 - val_loss: 0.5667 - val_accuracy: 0.8710\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.6616 - accuracy: 0.8173 - val_loss: 0.5325 - val_accuracy: 0.8813\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.6326 - accuracy: 0.8289 - val_loss: 0.5011 - val_accuracy: 0.8904\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.6051 - accuracy: 0.8367 - val_loss: 0.4737 - val_accuracy: 0.8973\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.5744 - accuracy: 0.8497 - val_loss: 0.4487 - val_accuracy: 0.9048\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.5499 - accuracy: 0.8575 - val_loss: 0.4261 - val_accuracy: 0.9095\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 1s 5ms/step - loss: 0.5282 - accuracy: 0.8648 - val_loss: 0.4062 - val_accuracy: 0.9132\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 1s 6ms/step - loss: 0.5117 - accuracy: 0.8688 - val_loss: 0.3872 - val_accuracy: 0.9177\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.4880 - accuracy: 0.8744 - val_loss: 0.3701 - val_accuracy: 0.9210\n",
            "Training time: 0:00:28.421582\n",
            "Test score: 0.3700830340385437\n",
            "Test accuracy: 0.9209963083267212\n"
          ]
        }
      ],
      "source": [
        "train_model(model,\n",
        "            (x_train_lt5, y_train_lt5),\n",
        "            (x_test_lt5, y_test_lt5), num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_eeHb6RSN9v"
      },
      "source": [
        "Note that after a single epoch, we are already achieving results on classifying 0-4 that are comparable to those achieved on 5-9 after 5 full epochs.  This despite the fact the we are only \"fine-tuning\" the last layer of the network, and all the early layers have never seen what the digits 0-4 look like.\n",
        "\n",
        "Also, note that even though nearly all (590K/600K) of the *parameters* were trainable, the training time per epoch was still much reduced.  This is because the unfrozen part of the network was very shallow, making backpropagation faster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Meo1RJCDSN9w"
      },
      "source": [
        "## Exercise\n",
        "### To do:\n",
        "- Now write code to reverse this training process.  That is, you will train on the digits 0-4, and then finetune only the last layers on the digits 5-9."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train model 0-4"
      ],
      "metadata": {
        "id": "P1JIOKllgvy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# un-Freeze only the\n",
        "for l in feature_layers:\n",
        "    l.trainable = True"
      ],
      "metadata": {
        "id": "pVwgypakiI2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We create our model by combining the two sets of layers as follows\n",
        "model2 = Sequential(feature_layers + classification_layers)"
      ],
      "metadata": {
        "id": "D9ZDsan9hyYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's take a look\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fl3mK2KJhgKB",
        "outputId": "82eee65c-e01f-4986-a557-989a96186389"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               589952    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600165 (2.29 MB)\n",
            "Trainable params: 600165 (2.29 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "hSQGYJ6FSN9w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd81a9bd-4d99-493a-f5a0-1fa287755b55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (30596, 28, 28, 1)\n",
            "30596 train samples\n",
            "5139 test samples\n",
            "Epoch 1/20\n",
            "240/240 [==============================] - 3s 8ms/step - loss: 0.4678 - accuracy: 0.8798 - val_loss: 0.3425 - val_accuracy: 0.9255\n",
            "Epoch 2/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.4425 - accuracy: 0.8835 - val_loss: 0.3190 - val_accuracy: 0.9282\n",
            "Epoch 3/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.4200 - accuracy: 0.8887 - val_loss: 0.2977 - val_accuracy: 0.9317\n",
            "Epoch 4/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3989 - accuracy: 0.8919 - val_loss: 0.2797 - val_accuracy: 0.9344\n",
            "Epoch 5/20\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 0.3811 - accuracy: 0.8951 - val_loss: 0.2635 - val_accuracy: 0.9362\n",
            "Epoch 6/20\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 0.3687 - accuracy: 0.8965 - val_loss: 0.2498 - val_accuracy: 0.9375\n",
            "Epoch 7/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3552 - accuracy: 0.8998 - val_loss: 0.2374 - val_accuracy: 0.9397\n",
            "Epoch 8/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3382 - accuracy: 0.9027 - val_loss: 0.2264 - val_accuracy: 0.9410\n",
            "Epoch 9/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3297 - accuracy: 0.9048 - val_loss: 0.2170 - val_accuracy: 0.9430\n",
            "Epoch 10/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3190 - accuracy: 0.9084 - val_loss: 0.2085 - val_accuracy: 0.9443\n",
            "Epoch 11/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.3083 - accuracy: 0.9115 - val_loss: 0.2004 - val_accuracy: 0.9457\n",
            "Epoch 12/20\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 0.3038 - accuracy: 0.9128 - val_loss: 0.1935 - val_accuracy: 0.9467\n",
            "Epoch 13/20\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 0.2930 - accuracy: 0.9137 - val_loss: 0.1868 - val_accuracy: 0.9488\n",
            "Epoch 14/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.2884 - accuracy: 0.9142 - val_loss: 0.1814 - val_accuracy: 0.9500\n",
            "Epoch 15/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.2847 - accuracy: 0.9143 - val_loss: 0.1771 - val_accuracy: 0.9512\n",
            "Epoch 16/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.2776 - accuracy: 0.9180 - val_loss: 0.1728 - val_accuracy: 0.9519\n",
            "Epoch 17/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.2684 - accuracy: 0.9202 - val_loss: 0.1682 - val_accuracy: 0.9531\n",
            "Epoch 18/20\n",
            "240/240 [==============================] - 2s 7ms/step - loss: 0.2696 - accuracy: 0.9186 - val_loss: 0.1645 - val_accuracy: 0.9535\n",
            "Epoch 19/20\n",
            "240/240 [==============================] - 2s 9ms/step - loss: 0.2592 - accuracy: 0.9222 - val_loss: 0.1609 - val_accuracy: 0.9543\n",
            "Epoch 20/20\n",
            "240/240 [==============================] - 2s 8ms/step - loss: 0.2555 - accuracy: 0.9229 - val_loss: 0.1576 - val_accuracy: 0.9549\n",
            "Training time: 0:00:42.129849\n",
            "Test score: 0.15755824744701385\n",
            "Test accuracy: 0.9548550248146057\n"
          ]
        }
      ],
      "source": [
        "# Now, let's train our model on the digits 0,1,2,3,4\n",
        "\n",
        "train_model(model2,\n",
        "            (x_train_lt5, y_train_lt5),\n",
        "            (x_test_lt5, y_test_lt5), num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Freeze only the\n",
        "for l in feature_layers:\n",
        "    l.trainable = False"
      ],
      "metadata": {
        "id": "iLisrJ7gicUQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "virTYWj8iids",
        "outputId": "2937c254-7973-40b9-ad5c-b61bffc52e06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_2 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 24, 24, 32)        9248      \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 24, 24, 32)        0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 12, 12, 32)        0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 4608)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 128)               589952    \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 128)               0         \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 645       \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 5)                 0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 600165 (2.29 MB)\n",
            "Trainable params: 590597 (2.25 MB)\n",
            "Non-trainable params: 9568 (37.38 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now, let's train our model on the digits 5,6,7,8,9\n",
        "\n",
        "train_model(model2,\n",
        "            (x_train_gte5, y_train_gte5),\n",
        "            (x_test_gte5, y_test_gte5), num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TFywz7ni9HT",
        "outputId": "3c17a5c1-a7b1-45ca-977e-78d8a59ba8ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train shape: (29404, 28, 28, 1)\n",
            "29404 train samples\n",
            "4861 test samples\n",
            "Epoch 1/20\n",
            "230/230 [==============================] - 2s 6ms/step - loss: 1.7457 - accuracy: 0.4829 - val_loss: 1.4877 - val_accuracy: 0.5404\n",
            "Epoch 2/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 1.5033 - accuracy: 0.5170 - val_loss: 1.2709 - val_accuracy: 0.5717\n",
            "Epoch 3/20\n",
            "230/230 [==============================] - 2s 7ms/step - loss: 1.3222 - accuracy: 0.5549 - val_loss: 1.1026 - val_accuracy: 0.6104\n",
            "Epoch 4/20\n",
            "230/230 [==============================] - 1s 6ms/step - loss: 1.1775 - accuracy: 0.5897 - val_loss: 0.9674 - val_accuracy: 0.6490\n",
            "Epoch 5/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 1.0575 - accuracy: 0.6220 - val_loss: 0.8597 - val_accuracy: 0.6832\n",
            "Epoch 6/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.9654 - accuracy: 0.6494 - val_loss: 0.7748 - val_accuracy: 0.7134\n",
            "Epoch 7/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.8856 - accuracy: 0.6744 - val_loss: 0.7071 - val_accuracy: 0.7418\n",
            "Epoch 8/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.8246 - accuracy: 0.6974 - val_loss: 0.6532 - val_accuracy: 0.7673\n",
            "Epoch 9/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.7719 - accuracy: 0.7197 - val_loss: 0.6094 - val_accuracy: 0.7846\n",
            "Epoch 10/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.7227 - accuracy: 0.7411 - val_loss: 0.5737 - val_accuracy: 0.7994\n",
            "Epoch 11/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.6850 - accuracy: 0.7557 - val_loss: 0.5436 - val_accuracy: 0.8095\n",
            "Epoch 12/20\n",
            "230/230 [==============================] - 1s 6ms/step - loss: 0.6574 - accuracy: 0.7674 - val_loss: 0.5184 - val_accuracy: 0.8245\n",
            "Epoch 13/20\n",
            "230/230 [==============================] - 1s 6ms/step - loss: 0.6298 - accuracy: 0.7807 - val_loss: 0.4964 - val_accuracy: 0.8340\n",
            "Epoch 14/20\n",
            "230/230 [==============================] - 1s 6ms/step - loss: 0.6054 - accuracy: 0.7911 - val_loss: 0.4772 - val_accuracy: 0.8397\n",
            "Epoch 15/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5837 - accuracy: 0.8000 - val_loss: 0.4598 - val_accuracy: 0.8486\n",
            "Epoch 16/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5677 - accuracy: 0.8045 - val_loss: 0.4442 - val_accuracy: 0.8562\n",
            "Epoch 17/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5498 - accuracy: 0.8130 - val_loss: 0.4299 - val_accuracy: 0.8624\n",
            "Epoch 18/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5392 - accuracy: 0.8164 - val_loss: 0.4172 - val_accuracy: 0.8681\n",
            "Epoch 19/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5222 - accuracy: 0.8253 - val_loss: 0.4060 - val_accuracy: 0.8739\n",
            "Epoch 20/20\n",
            "230/230 [==============================] - 1s 5ms/step - loss: 0.5140 - accuracy: 0.8288 - val_loss: 0.3960 - val_accuracy: 0.8774\n",
            "Training time: 0:00:26.144453\n",
            "Test score: 0.3960272967815399\n",
            "Test accuracy: 0.8773914575576782\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}